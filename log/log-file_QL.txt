MDP à 12 états :
(0, 0),  (0, 1),  (0, 2),  (0, 3),  (1, 0),  (1, 1),  (1, 2),  (1, 3),  (2, 0),  (2, 1),  (2, 2),  (2, 3), 
Etats terminaux : [(1, 3), (2, 3)]
But: [(2, 3)]
Gammma : 0.99
Nombre d'épisodes : 5
Alpha : 0.5
====================================================================================================
		Q-Learning :
====================================================================================================
====================================
Episode 0 : 
====================================
------------------------------------
Step 0 : 
State : (0, 0)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),up) = Q((0, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),up) = Q((0, 0),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 1 : 
State : (0, 0)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),down) = Q((0, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),down) = Q((0, 0),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 2 : 
State : (0, 0)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),left) = Q((0, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),left) = Q((0, 0),left) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 3 : 
State : (0, 0)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (0, 1)
Reward : -0.04
Q((0, 0),right) = Q((0, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),right) = Q((0, 0),right) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 4 : 
State : (0, 1)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (0, 1)
Reward : -0.04
Q((0, 1),up) = Q((0, 1),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),up) = Q((0, 1),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 5 : 
State : (0, 1)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (0, 1)
Reward : -0.04
Q((0, 1),down) = Q((0, 1),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),down) = Q((0, 1),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 6 : 
State : (0, 1)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (0, 0)
Reward : -0.04
Q((0, 1),left) = Q((0, 1),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),left) = Q((0, 1),left) + 0.5x1.0x(-0.04 + 0.99x-0.02 - -0.029900000000000003) = -0.029900000000000003
------------------------------------
Step 7 : 
State : (0, 0)
Action : up
N(s,a) : {'up': 2, 'down': 1, 'left': 1, 'right': 1}
Next state : (1, 0)
Reward : -0.04
Q((0, 0),up) = Q((0, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),up) = Q((0, 0),up) + 0.5x0.5x(-0.04 + 0.99x0 - -0.025) = -0.025
------------------------------------
Step 8 : 
State : (1, 0)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),up) = Q((1, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),up) = Q((1, 0),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 9 : 
State : (2, 0)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (2, 0)
Reward : -0.04
Q((2, 0),up) = Q((2, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),up) = Q((2, 0),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 10 : 
State : (2, 0)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (1, 0)
Reward : -0.04
Q((2, 0),down) = Q((2, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),down) = Q((2, 0),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 11 : 
State : (1, 0)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (0, 0)
Reward : -0.04
Q((1, 0),down) = Q((1, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),down) = Q((1, 0),down) + 0.5x1.0x(-0.04 + 0.99x-0.02 - -0.029900000000000003) = -0.029900000000000003
------------------------------------
Step 12 : 
State : (0, 0)
Action : down
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 1}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),down) = Q((0, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),down) = Q((0, 0),down) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 13 : 
State : (0, 0)
Action : left
N(s,a) : {'up': 2, 'down': 2, 'left': 2, 'right': 1}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),left) = Q((0, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),left) = Q((0, 0),left) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 14 : 
State : (0, 0)
Action : right
N(s,a) : {'up': 2, 'down': 2, 'left': 2, 'right': 2}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),right) = Q((0, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),right) = Q((0, 0),right) + 0.5x0.5x(-0.04 + 0.99x-0.025 - -0.02995) = -0.02995
------------------------------------
Step 15 : 
State : (0, 0)
Action : up
N(s,a) : {'up': 3, 'down': 2, 'left': 2, 'right': 2}
Next state : (1, 0)
Reward : -0.04
Q((0, 0),up) = Q((0, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),up) = Q((0, 0),up) + 0.5x0.3333333333333333x(-0.04 + 0.99x0 - -0.0275) = -0.0275
------------------------------------
Step 16 : 
State : (1, 0)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (0, 0)
Reward : -0.04
Q((1, 0),left) = Q((1, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),left) = Q((1, 0),left) + 0.5x1.0x(-0.04 + 0.99x-0.0275 - -0.033612500000000003) = -0.033612500000000003
------------------------------------
Step 17 : 
State : (0, 0)
Action : up
N(s,a) : {'up': 4, 'down': 2, 'left': 2, 'right': 2}
Next state : (1, 0)
Reward : -0.04
Q((0, 0),up) = Q((0, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),up) = Q((0, 0),up) + 0.5x0.25x(-0.04 + 0.99x0 - -0.0290625) = -0.0290625
------------------------------------
Step 18 : 
State : (1, 0)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),right) = Q((1, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),right) = Q((1, 0),right) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 19 : 
State : (2, 0)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (2, 0)
Reward : -0.04
Q((2, 0),left) = Q((2, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),left) = Q((2, 0),left) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 20 : 
State : (2, 0)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 1)
Reward : -0.04
Q((2, 0),right) = Q((2, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),right) = Q((2, 0),right) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 21 : 
State : (2, 1)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (2, 1)
Reward : -0.04
Q((2, 1),up) = Q((2, 1),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 1),up) = Q((2, 1),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 22 : 
State : (2, 1)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (2, 2)
Reward : -0.04
Q((2, 1),down) = Q((2, 1),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 1),down) = Q((2, 1),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 23 : 
State : (2, 2)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (2, 2)
Reward : -0.04
Q((2, 2),up) = Q((2, 2),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 2),up) = Q((2, 2),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 24 : 
State : (2, 2)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (1, 2)
Reward : -0.04
Q((2, 2),down) = Q((2, 2),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 2),down) = Q((2, 2),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 25 : 
State : (1, 2)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (2, 2)
Reward : -0.04
Q((1, 2),up) = Q((1, 2),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 2),up) = Q((1, 2),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 26 : 
State : (2, 2)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (2, 2)
Reward : -0.04
Q((2, 2),left) = Q((2, 2),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 2),left) = Q((2, 2),left) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 27 : 
State : (2, 2)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 3)
Reward : 1
Q((2, 2),right) = Q((2, 2),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 2),right) = Q((2, 2),right) + 0.5x1.0x(1 + 0.99x0 - 0.5) = 0.5
====================================
Episode 1 : 
====================================
------------------------------------
Step 28 : 
State : (0, 0)
Action : up
N(s,a) : {'up': 5, 'down': 2, 'left': 2, 'right': 2}
Next state : (1, 0)
Reward : -0.04
Q((0, 0),up) = Q((0, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),up) = Q((0, 0),up) + 0.5x0.2x(-0.04 + 0.99x-0.02 - -0.032136250000000005) = -0.032136250000000005
------------------------------------
Step 29 : 
State : (1, 0)
Action : up
N(s,a) : {'up': 2, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),up) = Q((1, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),up) = Q((1, 0),up) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 30 : 
State : (2, 0)
Action : up
N(s,a) : {'up': 2, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 0)
Reward : -0.04
Q((2, 0),up) = Q((2, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),up) = Q((2, 0),up) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 31 : 
State : (2, 0)
Action : down
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 1}
Next state : (1, 0)
Reward : -0.04
Q((2, 0),down) = Q((2, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),down) = Q((2, 0),down) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 32 : 
State : (1, 0)
Action : right
N(s,a) : {'up': 2, 'down': 1, 'left': 1, 'right': 2}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),right) = Q((1, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),right) = Q((1, 0),right) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 33 : 
State : (2, 0)
Action : left
N(s,a) : {'up': 2, 'down': 2, 'left': 2, 'right': 1}
Next state : (2, 0)
Reward : -0.04
Q((2, 0),left) = Q((2, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),left) = Q((2, 0),left) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 34 : 
State : (2, 0)
Action : right
N(s,a) : {'up': 2, 'down': 2, 'left': 2, 'right': 2}
Next state : (2, 1)
Reward : -0.04
Q((2, 0),right) = Q((2, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),right) = Q((2, 0),right) + 0.5x0.5x(-0.04 + 0.99x0 - -0.025) = -0.025
------------------------------------
Step 35 : 
State : (2, 1)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (2, 0)
Reward : -0.04
Q((2, 1),left) = Q((2, 1),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 1),left) = Q((2, 1),left) + 0.5x1.0x(-0.04 + 0.99x-0.025 - -0.032375) = -0.032375
------------------------------------
Step 36 : 
State : (2, 0)
Action : right
N(s,a) : {'up': 2, 'down': 2, 'left': 2, 'right': 3}
Next state : (2, 1)
Reward : -0.04
Q((2, 0),right) = Q((2, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),right) = Q((2, 0),right) + 0.5x0.3333333333333333x(-0.04 + 0.99x0 - -0.0275) = -0.0275
------------------------------------
Step 37 : 
State : (2, 1)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 2)
Reward : -0.04
Q((2, 1),right) = Q((2, 1),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 1),right) = Q((2, 1),right) + 0.5x1.0x(-0.04 + 0.99x0.5 - 0.2275) = 0.2275
------------------------------------
Step 38 : 
State : (2, 2)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 2}
Next state : (2, 3)
Reward : 1
Q((2, 2),right) = Q((2, 2),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 2),right) = Q((2, 2),right) + 0.5x0.5x(1 + 0.99x0 - 0.625) = 0.625
====================================
Episode 2 : 
====================================
------------------------------------
Step 39 : 
State : (0, 0)
Action : down
N(s,a) : {'up': 5, 'down': 3, 'left': 2, 'right': 2}
Next state : (0, 1)
Reward : -0.04
Q((0, 0),down) = Q((0, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),down) = Q((0, 0),down) + 0.5x0.3333333333333333x(-0.04 + 0.99x0 - -0.031625) = -0.031625
------------------------------------
Step 40 : 
State : (0, 1)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (0, 2)
Reward : -0.04
Q((0, 1),right) = Q((0, 1),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),right) = Q((0, 1),right) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 41 : 
State : (0, 2)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (1, 2)
Reward : -0.04
Q((0, 2),up) = Q((0, 2),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),up) = Q((0, 2),up) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 42 : 
State : (1, 2)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (0, 2)
Reward : -0.04
Q((1, 2),down) = Q((1, 2),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 2),down) = Q((1, 2),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 43 : 
State : (0, 2)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (0, 2)
Reward : -0.04
Q((0, 2),down) = Q((0, 2),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),down) = Q((0, 2),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 44 : 
State : (0, 2)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (0, 1)
Reward : -0.04
Q((0, 2),left) = Q((0, 2),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),left) = Q((0, 2),left) + 0.5x1.0x(-0.04 + 0.99x-0.02 - -0.029900000000000003) = -0.029900000000000003
------------------------------------
Step 45 : 
State : (0, 1)
Action : up
N(s,a) : {'up': 2, 'down': 1, 'left': 1, 'right': 1}
Next state : (0, 1)
Reward : -0.04
Q((0, 1),up) = Q((0, 1),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),up) = Q((0, 1),up) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 46 : 
State : (0, 1)
Action : down
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 1}
Next state : (0, 1)
Reward : -0.04
Q((0, 1),down) = Q((0, 1),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),down) = Q((0, 1),down) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 47 : 
State : (0, 1)
Action : right
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 2}
Next state : (0, 2)
Reward : -0.04
Q((0, 1),right) = Q((0, 1),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),right) = Q((0, 1),right) + 0.5x0.5x(-0.04 + 0.99x0 - -0.025) = -0.025
------------------------------------
Step 48 : 
State : (0, 2)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (0, 3)
Reward : -0.04
Q((0, 2),right) = Q((0, 2),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),right) = Q((0, 2),right) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 49 : 
State : (0, 3)
Action : up
N(s,a) : {'up': 1, 'down': 0, 'left': 0, 'right': 0}
Next state : (1, 3)
Reward : -1
Q((0, 3),up) = Q((0, 3),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 3),up) = Q((0, 3),up) + 0.5x1.0x(-1 + 0.99x0 - -0.5) = -0.5
====================================
Episode 3 : 
====================================
------------------------------------
Step 50 : 
State : (0, 0)
Action : left
N(s,a) : {'up': 5, 'down': 3, 'left': 3, 'right': 2}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),left) = Q((0, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),left) = Q((0, 0),left) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.02995 - -0.03656675) = -0.03656675
------------------------------------
Step 51 : 
State : (0, 0)
Action : right
N(s,a) : {'up': 5, 'down': 3, 'left': 3, 'right': 3}
Next state : (0, 1)
Reward : -0.04
Q((0, 0),right) = Q((0, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),right) = Q((0, 0),right) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.025 - -0.035750000000000004) = -0.035750000000000004
------------------------------------
Step 52 : 
State : (0, 1)
Action : right
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 3}
Next state : (0, 2)
Reward : -0.04
Q((0, 1),right) = Q((0, 1),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),right) = Q((0, 1),right) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.02 - -0.0308) = -0.0308
------------------------------------
Step 53 : 
State : (0, 2)
Action : up
N(s,a) : {'up': 2, 'down': 1, 'left': 1, 'right': 1}
Next state : (1, 2)
Reward : -0.04
Q((0, 2),up) = Q((0, 2),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),up) = Q((0, 2),up) + 0.5x0.5x(-0.04 + 0.99x0 - -0.025) = -0.025
------------------------------------
Step 54 : 
State : (1, 2)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (1, 2)
Reward : -0.04
Q((1, 2),left) = Q((1, 2),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 2),left) = Q((1, 2),left) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 55 : 
State : (1, 2)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (1, 3)
Reward : -1
Q((1, 2),right) = Q((1, 2),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 2),right) = Q((1, 2),right) + 0.5x1.0x(-1 + 0.99x0 - -0.5) = -0.5
====================================
Episode 4 : 
====================================
------------------------------------
Step 56 : 
State : (0, 0)
Action : down
N(s,a) : {'up': 5, 'down': 4, 'left': 3, 'right': 3}
Next state : (0, 0)
Reward : -0.04
Q((0, 0),down) = Q((0, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),down) = Q((0, 0),down) + 0.5x0.25x(-0.04 + 0.99x-0.032136250000000005 - -0.03658546875) = -0.03658546875
------------------------------------
Step 57 : 
State : (0, 0)
Action : up
N(s,a) : {'up': 6, 'down': 4, 'left': 3, 'right': 3}
Next state : (1, 0)
Reward : -0.04
Q((0, 0),up) = Q((0, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),up) = Q((0, 0),up) + 0.5x0.16666666666666666x(-0.04 + 0.99x-0.029900000000000003 - -0.03525831250000001) = -0.03525831250000001
------------------------------------
Step 58 : 
State : (1, 0)
Action : down
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 2}
Next state : (1, 0)
Reward : -0.04
Q((1, 0),down) = Q((1, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),down) = Q((1, 0),down) + 0.5x0.5x(-0.04 + 0.99x-0.02995 - -0.03982525) = -0.03982525
------------------------------------
Step 59 : 
State : (1, 0)
Action : up
N(s,a) : {'up': 3, 'down': 2, 'left': 1, 'right': 2}
Next state : (1, 0)
Reward : -0.04
Q((1, 0),up) = Q((1, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),up) = Q((1, 0),up) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.02995 - -0.03656675) = -0.03656675
------------------------------------
Step 60 : 
State : (1, 0)
Action : right
N(s,a) : {'up': 3, 'down': 2, 'left': 1, 'right': 3}
Next state : (1, 0)
Reward : -0.04
Q((1, 0),right) = Q((1, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),right) = Q((1, 0),right) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.033612500000000003 - -0.03656675) = -0.03656675
------------------------------------
Step 61 : 
State : (1, 0)
Action : left
N(s,a) : {'up': 3, 'down': 2, 'left': 2, 'right': 3}
Next state : (1, 0)
Reward : -0.04
Q((1, 0),left) = Q((1, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),left) = Q((1, 0),left) + 0.5x0.5x(-0.04 + 0.99x-0.03656675 - -0.04352846875000001) = -0.04352846875000001
------------------------------------
Step 62 : 
State : (1, 0)
Action : up
N(s,a) : {'up': 4, 'down': 2, 'left': 2, 'right': 3}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),up) = Q((1, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),up) = Q((1, 0),up) + 0.5x0.25x(-0.04 + 0.99x-0.0275 - -0.04039903125) = -0.04039903125
------------------------------------
Step 63 : 
State : (2, 0)
Action : right
N(s,a) : {'up': 2, 'down': 2, 'left': 2, 'right': 4}
Next state : (1, 0)
Reward : -0.04
Q((2, 0),right) = Q((2, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),right) = Q((2, 0),right) + 0.5x0.25x(-0.04 + 0.99x-0.03656675 - -0.0335876353125) = -0.0335876353125
------------------------------------
Step 64 : 
State : (1, 0)
Action : right
N(s,a) : {'up': 4, 'down': 2, 'left': 2, 'right': 4}
Next state : (1, 0)
Reward : -0.04
Q((1, 0),right) = Q((1, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),right) = Q((1, 0),right) + 0.5x0.25x(-0.04 + 0.99x-0.03982525 - -0.0415210415625) = -0.0415210415625
------------------------------------
Step 65 : 
State : (1, 0)
Action : down
N(s,a) : {'up': 4, 'down': 3, 'left': 2, 'right': 4}
Next state : (0, 0)
Reward : -0.04
Q((1, 0),down) = Q((1, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),down) = Q((1, 0),down) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.03525831250000001 - -0.0456719965625) = -0.0456719965625
------------------------------------
Step 66 : 
State : (0, 0)
Action : up
N(s,a) : {'up': 7, 'down': 4, 'left': 3, 'right': 3}
Next state : (1, 0)
Reward : -0.04
Q((0, 0),up) = Q((0, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),up) = Q((0, 0),up) + 0.5x0.14285714285714285x(-0.04 + 0.99x-0.04039903125 - -0.03845379310267858) = -0.03845379310267858
------------------------------------
Step 67 : 
State : (1, 0)
Action : up
N(s,a) : {'up': 5, 'down': 3, 'left': 2, 'right': 4}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),up) = Q((1, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),up) = Q((1, 0),up) + 0.5x0.2x(-0.04 + 0.99x-0.02995 - -0.043324178125) = -0.043324178125
------------------------------------
Step 68 : 
State : (2, 0)
Action : up
N(s,a) : {'up': 3, 'down': 2, 'left': 2, 'right': 4}
Next state : (2, 0)
Reward : -0.04
Q((2, 0),up) = Q((2, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),up) = Q((2, 0),up) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.02995 - -0.03656675) = -0.03656675
------------------------------------
Step 69 : 
State : (2, 0)
Action : down
N(s,a) : {'up': 3, 'down': 3, 'left': 2, 'right': 4}
Next state : (1, 0)
Reward : -0.04
Q((2, 0),down) = Q((2, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),down) = Q((2, 0),down) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.0415210415625 - -0.0384759718578125) = -0.0384759718578125
------------------------------------
Step 70 : 
State : (1, 0)
Action : right
N(s,a) : {'up': 5, 'down': 3, 'left': 2, 'right': 5}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),right) = Q((1, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),right) = Q((1, 0),right) + 0.5x0.2x(-0.04 + 0.99x-0.02995 - -0.04433398740625) = -0.04433398740625
------------------------------------
Step 71 : 
State : (2, 0)
Action : left
N(s,a) : {'up': 3, 'down': 3, 'left': 3, 'right': 4}
Next state : (2, 0)
Reward : -0.04
Q((2, 0),left) = Q((2, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),left) = Q((2, 0),left) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.0335876353125 - -0.03656675) = -0.03656675
------------------------------------
Step 72 : 
State : (2, 0)
Action : right
N(s,a) : {'up': 3, 'down': 3, 'left': 3, 'right': 5}
Next state : (1, 0)
Reward : -0.04
Q((2, 0),right) = Q((2, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),right) = Q((2, 0),right) + 0.5x0.2x(-0.04 + 0.99x-0.043324178125 - -0.038517965415625) = -0.038517965415625
------------------------------------
Step 73 : 
State : (1, 0)
Action : up
N(s,a) : {'up': 6, 'down': 3, 'left': 2, 'right': 5}
Next state : (2, 0)
Reward : -0.04
Q((1, 0),up) = Q((1, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),up) = Q((1, 0),up) + 0.5x0.16666666666666666x(-0.04 + 0.99x-0.03656675 - -0.04606392015625) = -0.04606392015625
------------------------------------
Step 74 : 
State : (2, 0)
Action : up
N(s,a) : {'up': 4, 'down': 3, 'left': 3, 'right': 5}
Next state : (2, 0)
Reward : -0.04
Q((2, 0),up) = Q((2, 0),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),up) = Q((2, 0),up) + 0.5x0.25x(-0.04 + 0.99x-0.03656675 - -0.0415210415625) = -0.0415210415625
------------------------------------
Step 75 : 
State : (2, 0)
Action : left
N(s,a) : {'up': 4, 'down': 3, 'left': 4, 'right': 5}
Next state : (1, 0)
Reward : -0.04
Q((2, 0),left) = Q((2, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 0),left) = Q((2, 0),left) + 0.5x0.25x(-0.04 + 0.99x-0.04352846875000001 - -0.0423825542578125) = -0.0423825542578125
------------------------------------
Step 76 : 
State : (1, 0)
Action : left
N(s,a) : {'up': 6, 'down': 3, 'left': 3, 'right': 5}
Next state : (1, 0)
Reward : -0.04
Q((1, 0),left) = Q((1, 0),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),left) = Q((1, 0),left) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.04433398740625 - -0.05012258796875001) = -0.05012258796875001
------------------------------------
Step 77 : 
State : (1, 0)
Action : right
N(s,a) : {'up': 6, 'down': 3, 'left': 3, 'right': 6}
Next state : (1, 0)
Reward : -0.04
Q((1, 0),right) = Q((1, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),right) = Q((1, 0),right) + 0.5x0.16666666666666666x(-0.04 + 0.99x-0.0456719965625 - -0.04763037575007812) = -0.04763037575007812
------------------------------------
Step 78 : 
State : (1, 0)
Action : down
N(s,a) : {'up': 6, 'down': 4, 'left': 3, 'right': 6}
Next state : (0, 0)
Reward : -0.04
Q((1, 0),down) = Q((1, 0),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 0),down) = Q((1, 0),down) + 0.5x0.25x(-0.04 + 0.99x-0.035750000000000004 - -0.049387059492187496) = -0.049387059492187496
------------------------------------
Step 79 : 
State : (0, 0)
Action : right
N(s,a) : {'up': 7, 'down': 4, 'left': 3, 'right': 4}
Next state : (0, 1)
Reward : -0.04
Q((0, 0),right) = Q((0, 0),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 0),right) = Q((0, 0),right) + 0.5x0.25x(-0.04 + 0.99x-0.029900000000000003 - -0.039981375) = -0.039981375
------------------------------------
Step 80 : 
State : (0, 1)
Action : left
N(s,a) : {'up': 2, 'down': 2, 'left': 2, 'right': 3}
Next state : (0, 1)
Reward : -0.04
Q((0, 1),left) = Q((0, 1),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),left) = Q((0, 1),left) + 0.5x0.5x(-0.04 + 0.99x-0.02995 - -0.03982525) = -0.03982525
------------------------------------
Step 81 : 
State : (0, 1)
Action : up
N(s,a) : {'up': 3, 'down': 2, 'left': 2, 'right': 3}
Next state : (0, 1)
Reward : -0.04
Q((0, 1),up) = Q((0, 1),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),up) = Q((0, 1),up) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.02995 - -0.03656675) = -0.03656675
------------------------------------
Step 82 : 
State : (0, 1)
Action : down
N(s,a) : {'up': 3, 'down': 3, 'left': 2, 'right': 3}
Next state : (0, 1)
Reward : -0.04
Q((0, 1),down) = Q((0, 1),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),down) = Q((0, 1),down) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.0308 - -0.03656675) = -0.03656675
------------------------------------
Step 83 : 
State : (0, 1)
Action : right
N(s,a) : {'up': 3, 'down': 3, 'left': 2, 'right': 4}
Next state : (0, 2)
Reward : -0.04
Q((0, 1),right) = Q((0, 1),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 1),right) = Q((0, 1),right) + 0.5x0.25x(-0.04 + 0.99x-0.02 - -0.034425000000000004) = -0.034425000000000004
------------------------------------
Step 84 : 
State : (0, 2)
Action : down
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 1}
Next state : (0, 3)
Reward : -0.04
Q((0, 2),down) = Q((0, 2),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),down) = Q((0, 2),down) + 0.5x0.5x(-0.04 + 0.99x0 - -0.025) = -0.025
------------------------------------
Step 85 : 
State : (0, 3)
Action : down
N(s,a) : {'up': 1, 'down': 1, 'left': 0, 'right': 0}
Next state : (0, 3)
Reward : -0.04
Q((0, 3),down) = Q((0, 3),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 3),down) = Q((0, 3),down) + 0.5x1.0x(-0.04 + 0.99x0 - -0.02) = -0.02
------------------------------------
Step 86 : 
State : (0, 3)
Action : left
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 0}
Next state : (0, 2)
Reward : -0.04
Q((0, 3),left) = Q((0, 3),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 3),left) = Q((0, 3),left) + 0.5x1.0x(-0.04 + 0.99x-0.02 - -0.029900000000000003) = -0.029900000000000003
------------------------------------
Step 87 : 
State : (0, 2)
Action : right
N(s,a) : {'up': 2, 'down': 2, 'left': 1, 'right': 2}
Next state : (0, 3)
Reward : -0.04
Q((0, 2),right) = Q((0, 2),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),right) = Q((0, 2),right) + 0.5x0.5x(-0.04 + 0.99x0 - -0.025) = -0.025
------------------------------------
Step 88 : 
State : (0, 3)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 1}
Next state : (0, 3)
Reward : -0.04
Q((0, 3),right) = Q((0, 3),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 3),right) = Q((0, 3),right) + 0.5x1.0x(-0.04 + 0.99x-0.02 - -0.02) = -0.02
------------------------------------
Step 89 : 
State : (0, 3)
Action : down
N(s,a) : {'up': 1, 'down': 2, 'left': 1, 'right': 1}
Next state : (0, 3)
Reward : -0.04
Q((0, 3),down) = Q((0, 3),down) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 3),down) = Q((0, 3),down) + 0.5x0.5x(-0.04 + 0.99x-0.02 - -0.02995) = -0.02995
------------------------------------
Step 90 : 
State : (0, 3)
Action : right
N(s,a) : {'up': 1, 'down': 2, 'left': 1, 'right': 2}
Next state : (0, 3)
Reward : -0.04
Q((0, 3),right) = Q((0, 3),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 3),right) = Q((0, 3),right) + 0.5x0.5x(-0.04 + 0.99x-0.029900000000000003 - -0.02995) = -0.02995
------------------------------------
Step 91 : 
State : (0, 3)
Action : left
N(s,a) : {'up': 1, 'down': 2, 'left': 2, 'right': 2}
Next state : (0, 2)
Reward : -0.04
Q((0, 3),left) = Q((0, 3),left) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 3),left) = Q((0, 3),left) + 0.5x0.5x(-0.04 + 0.99x-0.025 - -0.0386125) = -0.0386125
------------------------------------
Step 92 : 
State : (0, 2)
Action : up
N(s,a) : {'up': 3, 'down': 2, 'left': 1, 'right': 2}
Next state : (1, 2)
Reward : -0.04
Q((0, 2),up) = Q((0, 2),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((0, 2),up) = Q((0, 2),up) + 0.5x0.3333333333333333x(-0.04 + 0.99x-0.02 - -0.0308) = -0.0308
------------------------------------
Step 93 : 
State : (1, 2)
Action : up
N(s,a) : {'up': 2, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 2)
Reward : -0.04
Q((1, 2),up) = Q((1, 2),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 2),up) = Q((1, 2),up) + 0.5x0.5x(-0.04 + 0.99x0.625 - 0.1296875) = 0.1296875
------------------------------------
Step 94 : 
State : (2, 2)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 3}
Next state : (1, 2)
Reward : -0.04
Q((2, 2),right) = Q((2, 2),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 2),right) = Q((2, 2),right) + 0.5x0.3333333333333333x(-0.04 + 0.99x0.1296875 - 0.5355651041666667) = 0.5355651041666667
------------------------------------
Step 95 : 
State : (1, 2)
Action : up
N(s,a) : {'up': 3, 'down': 1, 'left': 1, 'right': 1}
Next state : (2, 2)
Reward : -0.04
Q((1, 2),up) = Q((1, 2),up) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((1, 2),up) = Q((1, 2),up) + 0.5x0.3333333333333333x(-0.04 + 0.99x0.5355651041666667 - 0.1897744921875) = 0.1897744921875
------------------------------------
Step 96 : 
State : (2, 2)
Action : right
N(s,a) : {'up': 1, 'down': 1, 'left': 1, 'right': 4}
Next state : (2, 3)
Reward : 1
Q((2, 2),right) = Q((2, 2),right) + α/N(s,a) x (R + γxmax(Q(s',a')) - Q(s,a))
Q((2, 2),right) = Q((2, 2),right) + 0.5x0.25x(1 + 0.99x0 - 0.5936194661458334) = 0.5936194661458334
====================================
Update Politic et utility
====================================
π((0, 0)) = argmax_a Q(s,a) = left
U((0, 0)) = max_a Q(s,a) = -0.03656675
------------------------------------
π((0, 1)) = argmax_a Q(s,a) = right
U((0, 1)) = max_a Q(s,a) = -0.034425000000000004
------------------------------------
π((0, 2)) = argmax_a Q(s,a) = down
U((0, 2)) = max_a Q(s,a) = -0.025
------------------------------------
π((0, 3)) = argmax_a Q(s,a) = down
U((0, 3)) = max_a Q(s,a) = -0.02995
------------------------------------
π((1, 0)) = argmax_a Q(s,a) = up
U((1, 0)) = max_a Q(s,a) = -0.04606392015625
------------------------------------
(1, 1) is a Wall
------------------------------------
π((1, 2)) = argmax_a Q(s,a) = up
U((1, 2)) = max_a Q(s,a) = 0.1897744921875
------------------------------------
(1, 3) is an End State
------------------------------------
π((2, 0)) = argmax_a Q(s,a) = down
U((2, 0)) = max_a Q(s,a) = -0.0384759718578125
------------------------------------
π((2, 1)) = argmax_a Q(s,a) = right
U((2, 1)) = max_a Q(s,a) = 0.2275
------------------------------------
π((2, 2)) = argmax_a Q(s,a) = right
U((2, 2)) = max_a Q(s,a) = 0.5936194661458334
------------------------------------
(2, 3) is an End State
------------------------------------
